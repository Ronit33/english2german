{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6YKwvdQJrJD1"
      },
      "outputs": [],
      "source": [
        "!pip install -qU portalocker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IHHRpvIuWO3n"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJm12o85N6wG",
        "outputId": "cd534056-f7e6-469e-c84f-5ddd4bb058c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data/data.txt.zip\n",
            "  inflating: data/deu.txt            \n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!cp \"/content/drive/MyDrive/KAGGLE DATASETS/english2german/deu.txt.zip\" \"data/data.txt.zip\"\n",
        "!unzip \"data/data.txt.zip\" -d \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QHorcdFXWUjZ"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qNPnHqkfWpqQ"
      },
      "outputs": [],
      "source": [
        "f = open(\"data/deu.txt\", \"r\")\n",
        "data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKEPc5EKYjYi",
        "outputId": "85e80478-d8aa-436d-b79e-2d0958bda909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)',\n",
              " 'Hi.\\tHallo!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)',\n",
              " 'Hi.\\tGrüß Gott!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)',\n",
              " 'Run!\\tLauf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)',\n",
              " 'Run.\\tLauf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "line_split = data.split(\"\\n\")\n",
        "line_split[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFl0NC-dhI61"
      },
      "source": [
        "# Fixing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFrsJqroYB7P",
        "outputId": "a92fc023-3016-4e10-8c4a-920bb3f53229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['']\n",
            "English and german sentences\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221533, 221533)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "eng_sents = []\n",
        "ger_sents = []\n",
        "for line in line_split:\n",
        "  try:\n",
        "    english_text, german_text, _ = line.split(\"\\t\")\n",
        "    eng_sents.append(english_text)\n",
        "    ger_sents.append(german_text)\n",
        "  except:\n",
        "    print(line.split(\"\\t\"))\n",
        "\n",
        "print(\"English and german sentences\")\n",
        "len(eng_sents), len(ger_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ4jLdTXYCn2",
        "outputId": "81adcd62-3078-4e0b-8295-731017d41bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Go.',\n",
              "  'Hi.',\n",
              "  'Hi.',\n",
              "  'Run!',\n",
              "  'Run.',\n",
              "  'Wow!',\n",
              "  'Wow!',\n",
              "  'Fire!',\n",
              "  'Help!',\n",
              "  'Help!'],\n",
              " ['Geh.',\n",
              "  'Hallo!',\n",
              "  'Grüß Gott!',\n",
              "  'Lauf!',\n",
              "  'Lauf!',\n",
              "  'Potzdonner!',\n",
              "  'Donnerwetter!',\n",
              "  'Feuer!',\n",
              "  'Hilfe!',\n",
              "  'Zu Hülf!'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "eng_sents[:10], ger_sents[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5seMXDqWcKl0",
        "outputId": "e73192aa-e0f4-4010-fc2a-4ea608e654a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-21 14:06:12.026948: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-10-21 14:06:12.081985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-21 14:06:13.201742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting de-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O6BEMJ9b7aQ",
        "outputId": "109d2e3e-3e80-4c59-b9f3-9a11966dd2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-21 14:06:26.423001: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-10-21 14:06:26.477092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-21 14:06:27.590917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXJpXCgyhEFh"
      },
      "source": [
        "# Building Tokenizer and Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pQD9LUTCcpnY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f8wzNQOAdE-S"
      },
      "outputs": [],
      "source": [
        "english_tokenizer = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
        "german_tokenizer = get_tokenizer(\"spacy\", \"de_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUzBSZSgdRTR",
        "outputId": "574b9159-b438-4516-c220-a8be435ccbc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'this', 'is', 'ronit', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "english_tokenizer(\"Hello, this is ronit!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzlZbQrEdXfI",
        "outputId": "92c729eb-9b06-4064-b07f-1170742e17a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hallo', ',', 'das', 'ist', 'Ronit', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "german_tokenizer(\"Hallo, das ist Ronit!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2gZ3BNlgdf2R"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data, tokenizer):\n",
        "  for sent in data:\n",
        "    yield tokenizer(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x_JMLZUfgEo_"
      },
      "outputs": [],
      "source": [
        "english_vocab = build_vocab_from_iterator(\n",
        "    build_vocab(eng_sents, english_tokenizer),\n",
        "    specials=[\"<pad>\", \"<unk>\"]\n",
        ")\n",
        "english_vocab.set_default_index(english_vocab[\"<unk>\"])\n",
        "\n",
        "german_vocab = build_vocab_from_iterator(\n",
        "    build_vocab(ger_sents, german_tokenizer),\n",
        "    specials=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        ")\n",
        "german_vocab.set_default_index(german_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTMjgpCrgP0J",
        "outputId": "c8b832dc-2b31-4dca-834c-c7f255ff1d29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18014, 38614)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(english_vocab), len(german_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5qylfzXgSmQ",
        "outputId": "4b7758fd-8c3d-4215-d16d-dd652ea45b3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Hello', 'how', 'are', 'you', '?', ',', 'I', 'am', 'Ronit', '!'],\n",
              " [3480, 120, 37, 6, 8, 18, 3, 162, 1, 185],\n",
              " ['Hello', 'how', 'are', 'you', '?', ',', 'I', 'am', '<unk>', '!'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "english_example = english_tokenizer(\"Hello how are you?, I am Ronit!\")\n",
        "indexes = english_vocab(english_example)\n",
        "english_example, indexes, english_vocab.lookup_tokens(indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mimdxkYgfO3",
        "outputId": "9dec4fa2-f197-417a-c057-4431af058fed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<sos>',\n",
              "  'Hallo',\n",
              "  ',',\n",
              "  'wie',\n",
              "  'geht',\n",
              "  'es',\n",
              "  'dir',\n",
              "  '?',\n",
              "  'Ich',\n",
              "  'bin',\n",
              "  'Ronit',\n",
              "  '!',\n",
              "  '<eos>'],\n",
              " [2, 2083, 5, 59, 154, 19, 56, 8, 7, 48, 1, 24, 3],\n",
              " ['<sos>',\n",
              "  'Hallo',\n",
              "  ',',\n",
              "  'wie',\n",
              "  'geht',\n",
              "  'es',\n",
              "  'dir',\n",
              "  '?',\n",
              "  'Ich',\n",
              "  'bin',\n",
              "  '<unk>',\n",
              "  '!',\n",
              "  '<eos>'])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "german_example = german_tokenizer(\"Hallo, wie geht es dir? Ich bin Ronit!\")\n",
        "german_example.append(\"<eos>\")\n",
        "german_example.insert(0, \"<sos>\")\n",
        "indexes = german_vocab(german_example)\n",
        "german_example, indexes, german_vocab.lookup_tokens(indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT8uSpagoAvk"
      },
      "source": [
        "# Generating Batches and vectorizing text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP-gW1H6g7u8",
        "outputId": "71793f1f-b17f-477a-f968-28815479e656"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Go.', 'Geh.'],\n",
              " ['Hi.', 'Hallo!'],\n",
              " ['Hi.', 'Grüß Gott!'],\n",
              " ['Run!', 'Lauf!'],\n",
              " ['Run.', 'Lauf!']]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "sentences = to_map_style_dataset(zip(eng_sents, ger_sents))\n",
        "# german_sentences = to_map_style_dataset(ger_sents)\n",
        "sentences = [list(sent) for sent in sentences]\n",
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "u0rd09F2rPhL"
      },
      "outputs": [],
      "source": [
        "def vectorize_batch(batch):\n",
        "\n",
        "  english_sents = [sents[0] for sents in batch]\n",
        "  german_sents = [sents[1] for sents in batch]\n",
        "\n",
        "  english_max_len=max([len(english_tokenizer(sent)) for sent in english_sents])\n",
        "\n",
        "  english_tokens = [english_vocab(english_tokenizer(sent)) for sent in english_sents]\n",
        "  english_tokens = [tokens + ([0]*(english_max_len-len(tokens))) for tokens in english_tokens]\n",
        "\n",
        "  german_max_len=max([len(german_tokenizer(sent)) for sent in german_sents])\n",
        "\n",
        "  german_tokens_input = [german_vocab(german_tokenizer(sent)) for sent in german_sents]\n",
        "  german_tokens_input = [german_vocab([\"<sos>\"])+tokens + german_vocab([\"<eos>\"]) + ([0]*(german_max_len-len(tokens))) for tokens in german_tokens_input]\n",
        "\n",
        "  return (torch.tensor(english_tokens, dtype=torch.int32),\n",
        "  torch.tensor(german_tokens_input, dtype=torch.long))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p4GGsEJVhUZ",
        "outputId": "6be4852b-1080-4a30-8c75-8c2a5d41972f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([635,   2], dtype=torch.int32)\n",
            "['Go', '.']\n",
            "tensor([3023,    2], dtype=torch.int32)\n",
            "['Hi', '.']\n",
            "tensor([3023,    2], dtype=torch.int32)\n",
            "['Hi', '.']\n",
            "tensor([5413,  185], dtype=torch.int32)\n",
            "['Run', '!']\n",
            "tensor([5413,    2], dtype=torch.int32)\n",
            "['Run', '.']\n"
          ]
        }
      ],
      "source": [
        "eng, ger = vectorize_batch(sentences[:5])\n",
        "for s in eng:\n",
        "  print(s)\n",
        "  print(english_vocab.lookup_tokens(s.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7dWMtiNi1aX",
        "outputId": "db9680fc-5ee4-4922-d9a2-b720fc90eb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  2, 761,   4,   3,   0])\n",
            "['<sos>', 'Geh', '.', '<eos>', '<pad>']\n",
            "tensor([   2, 2083,   24,    3,    0])\n",
            "['<sos>', 'Hallo', '!', '<eos>', '<pad>']\n",
            "tensor([   2, 5882, 1636,   24,    3])\n",
            "['<sos>', 'Grüß', 'Gott', '!', '<eos>']\n",
            "tensor([   2, 5351,   24,    3,    0])\n",
            "['<sos>', 'Lauf', '!', '<eos>', '<pad>']\n",
            "tensor([   2, 5351,   24,    3,    0])\n",
            "['<sos>', 'Lauf', '!', '<eos>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "for s in ger:\n",
        "  print(s)\n",
        "  print(german_vocab.lookup_tokens(s.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9QUSLuoZWPj_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(sentences, batch_size=256, collate_fn=vectorize_batch, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gfw7g5_eN6h",
        "outputId": "7aaf17d3-acd6-4033-d231-8611510430e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 16]) torch.Size([256, 22])\n",
            "torch.Size([256, 25]) torch.Size([256, 26])\n",
            "torch.Size([256, 20]) torch.Size([256, 27])\n",
            "torch.Size([256, 19]) torch.Size([256, 25])\n",
            "torch.Size([256, 21]) torch.Size([256, 21])\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "for eng, ger_in in train_loader:\n",
        "  i+=1\n",
        "  print(eng.shape, ger_in.shape)\n",
        "  if i==5: break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TIRythSfVnB"
      },
      "source": [
        "# Saving the dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "BCeAZjzMoM6G"
      },
      "outputs": [],
      "source": [
        "torch.save(train_loader, \"train_loader.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cMB8ghohoSU_"
      },
      "outputs": [],
      "source": [
        "saved_loader = torch.load(\"train_loader.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw9_bEGMoYMT",
        "outputId": "64edfbe2-b0d4-43b0-d058-ce234850a766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 23]) torch.Size([256, 27])\n",
            "torch.Size([256, 18]) torch.Size([256, 23])\n",
            "torch.Size([256, 23]) torch.Size([256, 30])\n",
            "torch.Size([256, 24]) torch.Size([256, 28])\n",
            "torch.Size([256, 17]) torch.Size([256, 21])\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "for eng, ger_in in saved_loader:\n",
        "  i+=1\n",
        "  print(eng.shape, ger_in.shape)\n",
        "  if i==5: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLuj16FdZKND"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OhMemxVCZfkg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bga9R53KkkQh",
        "outputId": "5f5071d5-ac1e-4228-ec3f-37b07cf0e714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 25, 128])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([256, 25, 75]), torch.Size([2, 256, 75]), torch.Size([2, 256, 75]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# [batch, seq]\n",
        "a = torch.randn(256, 25, 128)\n",
        "print(a.shape)\n",
        "ls = nn.LSTM(25, 75, num_layers=2, batch_first=True)\n",
        "out, (h, c) = ls(a)\n",
        "# [batch, seq], [1, hidden], [1, hidden]\n",
        "out.shape, h.shape, c.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "T8cUHXxUetxr"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mNssY_ZaRL"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uMl7I3nbZLz9"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(input_dim, embed_dim)\n",
        "    self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, src):\n",
        "    # src: [batch, seq]\n",
        "    embeddings = self.dropout(self.embedding_layer(src))\n",
        "\n",
        "    # [batch, seq, embed], [1, hidden, embed], [1, hidden, embed]\n",
        "    out, (h, c) = self.rnn(embeddings)\n",
        "    # out: [batch, seq, hidden*n_direction]\n",
        "    # h: [n_layers*n_direction, batch, hidden]\n",
        "    # c: [n_layers*n_direction, batch, hidden]\n",
        "\n",
        "    return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R3mKopSPb8OU"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding_layer = nn.Embedding(output_dim, embed_dim)\n",
        "    self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "    self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "\n",
        "    # h: [n_layers*n_direction, batch, hidden]\n",
        "    # c: [n_layers*n_direction, batch, hidden]\n",
        "\n",
        "\n",
        "    # input: [batch_size]-->[1, batch_size]\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embeddings = self.dropout(self.embedding_layer(input))\n",
        "    # [1, batch, embed_dim]\n",
        "    embeddings = embeddings.view(embeddings.size(1), embeddings.size(0), -1)\n",
        "    # print(\"in decoder\", embeddings.shape, hidden.shape, cell.shape)\n",
        "    out, (h, c) = self.rnn(embeddings, (hidden, cell))\n",
        "\n",
        "    prediction = self.linear(out.squeeze(0))\n",
        "\n",
        "\n",
        "    return prediction, h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-DODTvykBihF"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wtW6K7ove6sR"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
        "      'hidden dimensions of encoder and decoder must be equal.'\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\n",
        "      'n_layers of encoder and decoder must be equal.'\n",
        "\n",
        "  def forward(self, src, trg, teacher_force_ratio=0.5):\n",
        "\n",
        "    # src: [batch, seq]\n",
        "    # trg: [batch, seq]\n",
        "\n",
        "    trg_len = trg.shape[1]\n",
        "    batch_size = trg.shape[0]\n",
        "\n",
        "    # tensor to store decoder outputs\n",
        "    outputs = torch.zeros(batch_size, trg_len, len(german_vocab)).to(self.device)\n",
        "\n",
        "    h, c = self.encoder(src)\n",
        "\n",
        "    input = trg[:, 0]\n",
        "    for t in range(1, trg_len):\n",
        "      # print(input.shape, h.shape, c.shape)\n",
        "      # print(\"-\"*50)\n",
        "      out, h, c = self.decoder(input, h, c)\n",
        "      outputs[:, t, :] = out[:, 0, :]\n",
        "\n",
        "      teacher_force = random.random()<teacher_force_ratio\n",
        "\n",
        "      # get the highest predicted token from our predictions.\n",
        "      # top1 = out.argmax(1)\n",
        "\n",
        "      # update input : use ground_truth when teacher_force\n",
        "      # input = trg[: t] if teacher_force else top1\n",
        "      input = trg[:, t]\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3iMQv3hkkYeS"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(english_vocab)\n",
        "OUTPUT_DIM = len(german_vocab)\n",
        "ENC_EMBED_DIM = 256\n",
        "DEC_EMBED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENC_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmhw2IJACxje",
        "outputId": "06b5ec47-2202-44a4-a524-6a76ed2299fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding_layer): Embedding(18014, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding_layer): Embedding(38614, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "    (linear): Linear(in_features=512, out_features=38614, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "  for name, params in m.named_parameters():\n",
        "    nn.init.uniform_(params.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPFg1KKDsIi",
        "outputId": "029a6ca7-8238-4c79-b356-b412ff6156a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 41,662,166\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Number of trainable parameters: {count_parameters(model):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7qt0ASIEOSx"
      },
      "source": [
        "# Loss and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LnviqxbyEHsa"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    params=model.parameters()\n",
        ")\n",
        "\n",
        "TRG_PAD_IDX = german_vocab.lookup_indices([\"<pad>\"])\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE5NCzS8Fk1w"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DWOUPwLNEttA"
      },
      "outputs": [],
      "source": [
        "def train(model, data, criterion, optimizer, clip):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, batch in enumerate(data):\n",
        "    src, trg = batch\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # trg: [batch, seq]\n",
        "    # src = [batch, seq]\n",
        "    # print(src.shape, trg.shape, \"src and target\")\n",
        "    src = src.to(device)\n",
        "    trg = trg.to(device)\n",
        "    output = model(src, trg)\n",
        "\n",
        "    output_dim = output.shape[-1]\n",
        "\n",
        "    # now slice off the first column from both output and target\n",
        "\n",
        "    output = output[1:].view(-1, output_dim)\n",
        "    trg = trg[1:].view(-1)\n",
        "    trg.to(torch.long)\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "    if i%100==0:\n",
        "      print(loss.item())\n",
        "    epoch_loss+=loss.item()\n",
        "\n",
        "  return epoch_loss / len(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "MHILYuFxHrrw"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time  / 60)\n",
        "  elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
        "  return  elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ec3uWYqEH7Jw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aBotYQCHi2F",
        "outputId": "254da074-b42c-43c7-c4fa-6a76cfeac1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.55378246307373\n",
            "5.762933731079102\n",
            "5.381701469421387\n",
            "5.128652572631836\n",
            "5.065828800201416\n",
            "4.699405193328857\n",
            "4.571733474731445\n",
            "4.618707180023193\n",
            "4.444691181182861\n",
            "Epoch: 01 | Time 7m 4s\n",
            "\tTrain Loss: 5.052 | Train PPL: 156.299\n",
            "4.200536251068115\n",
            "4.142460346221924\n",
            "4.129533290863037\n",
            "4.046777725219727\n",
            "3.940574884414673\n",
            "4.115660190582275\n",
            "3.9681921005249023\n",
            "3.847494602203369\n",
            "3.7532436847686768\n",
            "Epoch: 02 | Time 7m 3s\n",
            "\tTrain Loss: 4.033 | Train PPL:  56.432\n",
            "3.6573238372802734\n",
            "3.5768706798553467\n",
            "3.538433790206909\n",
            "3.3904600143432617\n",
            "3.6309590339660645\n",
            "3.4876420497894287\n",
            "3.3868696689605713\n",
            "3.396064043045044\n",
            "3.3146250247955322\n",
            "Epoch: 03 | Time 7m 3s\n",
            "\tTrain Loss: 3.497 | Train PPL:  33.022\n",
            "3.241957187652588\n",
            "3.270005464553833\n",
            "3.1759088039398193\n",
            "3.068887948989868\n",
            "3.134232759475708\n",
            "3.1021978855133057\n",
            "2.978907585144043\n",
            "3.123380661010742\n",
            "2.962014675140381\n",
            "Epoch: 04 | Time 7m 4s\n",
            "\tTrain Loss: 3.094 | Train PPL:  22.060\n",
            "2.8101983070373535\n",
            "2.83697772026062\n",
            "2.6863787174224854\n",
            "2.910993814468384\n",
            "2.7871017456054688\n",
            "2.773364782333374\n",
            "2.7923800945281982\n",
            "2.7233381271362305\n",
            "2.8369319438934326\n",
            "Epoch: 05 | Time 7m 3s\n",
            "\tTrain Loss: 2.793 | Train PPL:  16.329\n",
            "2.576258659362793\n",
            "2.5698533058166504\n",
            "2.564352512359619\n",
            "2.5712461471557617\n",
            "2.626221179962158\n",
            "2.4922728538513184\n",
            "2.4441022872924805\n",
            "2.600270986557007\n",
            "2.5692806243896484\n",
            "Epoch: 06 | Time 7m 2s\n",
            "\tTrain Loss: 2.566 | Train PPL:  13.011\n",
            "2.357732057571411\n",
            "2.305227279663086\n",
            "2.444396734237671\n",
            "2.4109559059143066\n",
            "2.341782569885254\n",
            "2.426358222961426\n",
            "2.349259376525879\n",
            "2.378843307495117\n",
            "2.3196794986724854\n",
            "Epoch: 07 | Time 7m 4s\n",
            "\tTrain Loss: 2.392 | Train PPL:  10.934\n",
            "2.2711400985717773\n",
            "2.1205809116363525\n",
            "2.2575156688690186\n",
            "2.244959831237793\n",
            "2.3342833518981934\n",
            "2.264864444732666\n",
            "2.3016157150268555\n",
            "2.2569806575775146\n",
            "2.2384631633758545\n",
            "Epoch: 08 | Time 7m 4s\n",
            "\tTrain Loss: 2.257 | Train PPL:   9.559\n",
            "2.1396334171295166\n",
            "2.2391257286071777\n",
            "2.149878978729248\n",
            "2.144026279449463\n",
            "2.1881773471832275\n",
            "2.1652116775512695\n",
            "2.132991313934326\n",
            "2.180555820465088\n",
            "2.139280080795288\n",
            "Epoch: 09 | Time 7m 5s\n",
            "\tTrain Loss: 2.152 | Train PPL:   8.605\n",
            "2.0491676330566406\n",
            "2.106722593307495\n",
            "2.0991175174713135\n",
            "2.06199049949646\n",
            "2.0897088050842285\n",
            "2.10377836227417\n",
            "2.122791290283203\n",
            "2.043881893157959\n",
            "2.0527853965759277\n",
            "Epoch: 10 | Time 7m 6s\n",
            "\tTrain Loss: 2.069 | Train PPL:   7.915\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "\n",
        "  train_loss = train(model, train_loader, loss_fn, optimizer, CLIP)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s\")\n",
        "  print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "P7HgY-EqXoZx"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'decoder_state_dict': decoder.state_dict(),\n",
        "            'seq2seq_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "            }, \"all.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), \"encoder.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder.pth\")\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "torch.save(optimizer.state_dict(), \"optimizer.pth\")"
      ],
      "metadata": {
        "id": "kDt8puz01TLf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(english_vocab, \"english_vocab.pth\")\n",
        "torch.save(german_vocab, \"german_vocab.pth\")\n",
        "torch.save(train_loader, \"train_loader.pth\")"
      ],
      "metadata": {
        "id": "c1XCvE_l11-I"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoder.eval()\n",
        "  with torch.inference_mode():\n",
        "    tokens = english_tokenizer(text)\n",
        "    token_ids = english_vocab.lookup_indices(tokens)\n",
        "    token_tensors = torch.tensor(token_ids).unsqueeze(0)\n",
        "\n",
        "    german_start = [\"<sos>\"]\n",
        "\n",
        "    german_start_id = german_vocab(german_start)\n",
        "    german_start_tensor = torch.tensor(german_start_id).unsqueeze(0)\n",
        "\n",
        "    hidden, cell = encoder(token_tensors.to(device))\n",
        "\n",
        "\n",
        "    pred = \"\"\n",
        "    i=0\n",
        "    while pred!=\"<eos>\":\n",
        "      prediction, hidden, cell = decoder(german_start_tensor.to(device), hidden, cell)\n",
        "\n",
        "      predictions = prediction.squeeze(0)\n",
        "\n",
        "      pred = torch.argmax(predictions)\n",
        "      pred = german_vocab.lookup_token(pred)\n",
        "      print(pred, end=\" \")\n",
        "\n",
        "      if i>6: break\n",
        "      i+=1\n"
      ],
      "metadata": {
        "id": "DMYHBLGd3h5c"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# google translation: Willkommen zurück\n",
        "predict(\"Welcome back.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEI-Pq4n62Cx",
        "outputId": "b6bfa106-3494-4f98-dcbe-d69b341b2e3d"
      },
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Willkommen zurück . . . . . . "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22f4GQOh7Kdj"
      },
      "execution_count": 98,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}